import cv2
import torch
import numpy as np
from tqdm import tqdm
from sklearn.cluster import KMeans
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
import os

# ========== åˆå§‹åŒ–è§†è§‰æ¨¡å‹ ==========
visual_model_id = "Qwen/Qwen2.5-VL-3B-Instruct"
vis_emb_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    visual_model_id, attn_implementation="eager"
)
processor = AutoProcessor.from_pretrained(visual_model_id)
vis_emb_model.visual.to("cuda").eval()

# ========== æå–å¸§å‘é‡ ==========
def extract_visual_embeddings(video_path, frame_interval=30):
    cap = cv2.VideoCapture(video_path)
    frames, embeddings = [], []
    idx = 0
    with torch.no_grad():
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if idx % frame_interval == 0:
                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                inputs = processor.image_processor(images=image, return_tensors="pt")
                pixel_values = inputs["pixel_values"].to("cuda")
                grid_thw = inputs["image_grid_thw"].to("cuda")
                vision_outputs = vis_emb_model.visual(pixel_values, grid_thw)
                emb = vision_outputs.mean(dim=0).squeeze(0).cpu().numpy()  # shape [2048]
                embeddings.append(emb)
                frames.append(idx)
                print(idx)
            idx += 1
    cap.release()
    return np.array(embeddings), np.array(frames)

# ========== èšç±»å¹¶åˆ‡å‰² ==========
def cluster_and_segment(video_path, embeddings, frames, method="kmeans", n_clusters=5):
    print("clusting")
    if method == "kmeans":
        clusterer = KMeans(n_clusters=n_clusters, random_state=42)
        labels = clusterer.fit_predict(embeddings)
    elif method == "dbscan":
        from sklearn.cluster import DBSCAN
        clusterer = DBSCAN(eps=1.5, min_samples=3)
        labels = clusterer.fit_predict(embeddings)
    else:
        raise ValueError("Unsupported clustering method")

    # ä¾æ®èšç±»ç»“æœæ‰¾åˆ°åˆ‡ç‚¹
    change_points = [frames[0]]
    for i in range(1, len(labels)):
        if labels[i] != labels[i - 1]:
            change_points.append(frames[i])
    change_points.append(frames[-1])
    return change_points, labels

# # ========== å¯¼å‡ºå­è§†é¢‘ ==========
# def export_segments(video_path, change_points, output_prefix="segment"):
#     cap = cv2.VideoCapture(video_path)
#     fps = int(cap.get(cv2.CAP_PROP_FPS))
#     h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#     w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#     fourcc = cv2.VideoWriter_fourcc(*"mp4v")
#
#     seg_id = 0
#     for i in range(len(change_points) - 1):
#         start, end = change_points[i], change_points[i + 1]
#         out = cv2.VideoWriter(f"{output_prefix}_{seg_id}.mp4", fourcc, fps, (w, h))
#         cap.set(cv2.CAP_PROP_POS_FRAMES, start)
#         for j in range(start, end):
#             ret, frame = cap.read()
#             if not ret:
#                 break
#             out.write(frame)
#         out.release()
#         seg_id += 1
#     cap.release()
#     print(f"âœ… Exported {seg_id} segments.")


def export_segments(
    video_path,
    change_points,
    output_prefix="segment",
    min_segment_sec=10,
    output_dir="segments",
):
    """å¯¼å‡ºè§†é¢‘ç‰‡æ®µï¼Œå¹¶è¿”å›ç‰‡æ®µçš„å…ƒä¿¡æ¯ã€‚

    è‹¥å•ä¸ªç‰‡æ®µå°äºé˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œåˆ™åˆå¹¶åˆ°ä¸‹ä¸€ä¸ªç‰‡æ®µã€‚è¿ç»­çŸ­ç‰‡æ®µä¼šæ•´ä½“
    åˆå¹¶åˆ°åä¸€ä¸ªé•¿ç‰‡æ®µä¸­ï¼Œå¹¶å°†è¾“å‡ºä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ä¸­ã€‚

    Args:
        video_path (str): è§†é¢‘è·¯å¾„ã€‚
        change_points (list[int]): åˆ‡å‰²å¸§å·åˆ—è¡¨ã€‚
        output_prefix (str): è¾“å‡ºæ–‡ä»¶åå‰ç¼€ã€‚
        min_segment_sec (float): æœ€å°ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå°äºè¯¥å€¼çš„ä¼šåˆå¹¶åˆ°ä¸‹ä¸€ä¸ªç‰‡æ®µã€‚
        output_dir (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆè‹¥ä¸å­˜åœ¨å°†è‡ªåŠ¨åˆ›å»ºï¼‰ã€‚

    Returns:
        list[dict]: æ¯ä¸ªç‰‡æ®µçš„å…ƒä¿¡æ¯ï¼ŒåŒ…å« ``path``ã€``start_frame``ã€``end_frame`` å’Œ ``fps``ã€‚
    """
    print("å¯¼å‡ºè§†é¢‘")

    # ===== åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ =====
    os.makedirs(output_dir, exist_ok=True)

    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    min_frames = int(fps * min_segment_sec)

    # ===== åˆå¹¶çŸ­ç‰‡æ®µçš„åˆ‡ç‚¹é€»è¾‘ =====
    merged_points = [change_points[0]]
    i = 1
    while i < len(change_points):
        start, end = change_points[i - 1], change_points[i]
        seg_len = end - start

        if seg_len < min_frames and i < len(change_points) - 1:
            # å½“å‰æ®µå¤ªçŸ­ â†’ åˆå¹¶åˆ°ä¸‹ä¸€ä¸ªæ®µ
            i += 1
            continue
        else:
            merged_points.append(change_points[i])
            i += 1

    # ===== å¯¼å‡ºè§†é¢‘é˜¶æ®µ =====
    segment_infos = []
    seg_id = 0
    for i in range(len(merged_points) - 1):
        start, end = merged_points[i], merged_points[i + 1]
        out_path = os.path.join(output_dir, f"{output_prefix}_{seg_id}.mp4")

        out = cv2.VideoWriter(out_path, fourcc, fps, (w, h))
        cap.set(cv2.CAP_PROP_POS_FRAMES, start)

        for j in range(start, end):
            ret, frame = cap.read()
            if not ret:
                break
            out.write(frame)

        out.release()
        print(f"ğŸ¬ Saved: {out_path}")
        start_sec = float(start) / fps if fps else 0.0
        end_sec = float(end) / fps if fps else start_sec
        segment_infos.append(
            {
                "path": out_path,
                "start_frame": start,
                "end_frame": end,
                "fps": fps,
                "segment_index": seg_id,
                "start_sec": start_sec,
                "end_sec": end_sec,
                "duration_sec": max(end_sec - start_sec, 0.0),
            }
        )
        seg_id += 1

    cap.release()
    print(f"âœ… Exported {seg_id} segments to folder: {os.path.abspath(output_dir)}")

    return segment_infos


# ========== ä¸»æµç¨‹ ==========
if __name__ == "__main__":
    video_path = "Demo_19s.mp4"
    embeddings, frames = extract_visual_embeddings(video_path, frame_interval=5)
    change_points, labels = cluster_and_segment(video_path, embeddings, frames, method="kmeans", n_clusters=10)
    export_segments(video_path, change_points)
